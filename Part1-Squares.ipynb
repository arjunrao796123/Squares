{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow\n",
    "np.random.seed(1212)\n",
    "from tensorflow import keras \n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import feature\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First take a look at the histogram of all the images in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def hist_plot(path):\n",
    "    '''\n",
    "    This function provides the average of the values for each channel in the image\n",
    "    \n",
    "    '''\n",
    "    valid_images = [\"jpg\",\"gif\",\"png\",\"tga\",\"bmp\"]\n",
    "    results = [] \n",
    "    size = []\n",
    "    y = []\n",
    "    label = 0\n",
    "    count=0\n",
    "    for i in os.listdir(path):\n",
    "\n",
    "            if i.split('.')[1] not in valid_images:\n",
    "                continue # to loop over all images you have on the directory\n",
    "            img = cv2.imread(path+i)\n",
    "            gray = cv2.imread(path+i,0)\n",
    "            corners = cv2.goodFeaturesToTrack(gray,4,0.2,2,useHarrisDetector=True)\n",
    "            if len(corners) !=4:\n",
    "                print(j)\n",
    "\n",
    "            #img1 = img.crop((corners[1][0][0], corners[1][0][0], corners[1][0][1], corners[1][0][1])).convert('RGB')\n",
    "            crners= np.unique(corners)\n",
    "            crners.sort()\n",
    "            size.append(img.shape[0])\n",
    "\n",
    "            img = img[int(crners[0]):int(crners[1]), int(crners[0]):int(crners[1])]\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "            count+=1\n",
    "            #img = cv2.resize(img, (150,150))\n",
    "            #img = np.array(img)[:, :, ::-1]\n",
    "            avg_color_per_row = np.average(img, axis=0)\n",
    "            avg_color = np.average(avg_color_per_row, axis=0)\n",
    "            results.append(avg_color)\n",
    "    np_results = np.array(results) # to make results a numpy array\n",
    "    plt.xlabel('intensity')\n",
    "    plt.ylabel('count')\n",
    "    plt.hist(np_results,color=['blue','green','red'])\n",
    "    plt.show() # to show the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plot('train/a/')\n",
    "hist_plot('train/b/')\n",
    "hist_plot('train/c/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Resize images and create labels for the image\n",
    "\n",
    "### Checking all valid types of the image and appending them to the arrays so that they can be used later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_data(path,images_fun=[],labels_fun=[],image_size=150):\n",
    "    '''\n",
    "    \n",
    "    input is the path to the folder containing subdirectories\n",
    "    \n",
    "    output is the images in an array and labels ina an array\n",
    "    if subdirectories are a,b,c label for images in the corresponding folders is 0,1,2\n",
    "    \n",
    "    '''\n",
    "    IMAGE_SIZE = (image_size, image_size)\n",
    "    valid_images = [\"jpg\",\"gif\",\"png\",\"tga\",\"bmp\"]\n",
    "    label = 0\n",
    "    rawImages=[]\n",
    "    features=[]\n",
    "    for i in os.listdir(path):\n",
    "        if os.path.isdir(path + i):\n",
    "            for j in os.listdir(path+i):\n",
    "                if j.split('.')[1] not in valid_images:\n",
    "                    continue\n",
    "                #img = Image.open(path+i+'/'+j).convert('RGB')\n",
    "                img = cv2.imread(path+i+'/'+j)\n",
    "                gray = cv2.imread(path+i+'/'+j,0)\n",
    "                corners = cv2.goodFeaturesToTrack(gray,4,0.2,2,useHarrisDetector=True)\n",
    "                if len(corners) !=4:\n",
    "                    print(j)\n",
    "\n",
    "                #img1 = img.crop((corners[1][0][0], corners[1][0][0], corners[1][0][1], corners[1][0][1])).convert('RGB')\n",
    "                crners= np.unique(corners)\n",
    "                crners.sort()\n",
    "                img = img[int(crners[0]):int(crners[1]), int(crners[0]):int(crners[1])]\n",
    "                #image = cv2.imread(path+i+'/'+j)\n",
    "                #image = np.array(img1)[:, :, ::-1]\n",
    "                #image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Append the image and its corresponding label to the output\n",
    "\n",
    "                #pixels = image_to_feature_vector(image)\n",
    "                #hist = extract_color_histogram(image)\n",
    "                #image = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n",
    "                image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                image = cv2.resize(image, IMAGE_SIZE,0,0,interpolation = cv2.INTER_NEAREST) \n",
    "                images_fun.append(image)\n",
    "                labels_fun.append(label)\n",
    "                # update the raw images, features, and labels matricies,\n",
    "                # respectively\n",
    "                #rawImages.append(pixels)\n",
    "                #features.append(hist)\n",
    "            label+=1\n",
    "    images_fun = np.array(images_fun, dtype = 'float32')\n",
    "    labels_fun = np.array(labels_fun, dtype = 'int32')   \n",
    "    #output = []       \n",
    "    #output.append((images, labels))\n",
    "    return images_fun,labels_fun,rawImages,features\n",
    "\n",
    "images,labels,rawImages,features = create_data('train/',images_fun=[],labels_fun=[])\n",
    "images_val,labels_val,raw_val,feat_val = create_data('val/',images_fun=[],labels_fun=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Display examples of dataset and view class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_examples(class_names, images, labels):\n",
    "    \"\"\"\n",
    "        Display 25 images from the images array with its corresponding labels\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.suptitle(\"Some examples of images of the dataset\", fontsize=16)\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.y\n",
    "        ticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "        plt.xlabel(class_names[labels[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "_, train_counts = np.unique(labels, return_counts=True)\n",
    "_, test_counts = np.unique(labels_val, return_counts=True)\n",
    "pd.DataFrame({'train': train_counts,\n",
    "                    'test': test_counts}, \n",
    "             index=class_names\n",
    "            ).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(train_counts,\n",
    "        explode=(0, 0, 0) , \n",
    "        labels=class_names,\n",
    "        autopct='%1.1f%%')\n",
    "plt.axis('equal')\n",
    "plt.title('Proportion of each observed category')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images \n",
    "class_names =['a','b','c']\n",
    "class_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n",
    "\n",
    "nb_classes = len(class_names)\n",
    "\n",
    "\n",
    "train_images = images / 255.0 \n",
    "test_images = images_val / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "combined = list(zip(train_images, labels))\n",
    "random.shuffle(combined)\n",
    "\n",
    "a, b= zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_examples(class_names, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline CNN\n",
    "\n",
    "### Attempt of a simple CNN model to do the predicitons on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports for the dataset and building our neural network\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "num_digits = 3\n",
    "# one-hot encoding using keras' numpy-related utilities\n",
    "y_train = tensorflow.keras.utils.to_categorical(labels, num_digits)\n",
    "y_val = tensorflow.keras.utils.to_categorical(labels_val, num_digits)\n",
    "\n",
    "X_train = np.array(images)\n",
    "X_test = np.array(images_val)\n",
    "X_train = X_train.reshape(X_train.shape[0], 150, 150, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 150, 150, 3)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "n_classes = 3\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = y_train\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
    "\n",
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layer\n",
    "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(150, 150, 3)))\n",
    "\n",
    "# convolutional layer\n",
    "model.add(Conv2D(100, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(100, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# flatten output of conv\n",
    "model.add(Flatten())\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# output layer\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# training the model for 15 epochs\n",
    "model.fit(X_train, Y_train, batch_size=16, epochs=15,validation_data=(X_test,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features with Vgg-16 and perform PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the VGG19, extract the features of the dataset. Then use PCA on the featurees to see if clusters of the dataset is being formed by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import ResNet101\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "\n",
    "#model = resnet50.ResNet50(weights='imagenet', include_top=False)\n",
    "model2 = VGG19(weights='imagenet',include_top= False,input_shape = (150,150,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_features = model2.predict(train_images)\n",
    "test_features = model2.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, x, y, z = train_features.shape\n",
    "n_test, x, y, z = test_features.shape\n",
    "numFeatures = x * y * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components = 7)\n",
    "\n",
    "X_test = test_features.reshape((n_train, x*y*z))\n",
    "\n",
    "pca.fit(X)\n",
    "\n",
    "C = pca.transform(X)\n",
    "C_test = pca.transform(X_test)\n",
    "\n",
    "C1 = C[:,0]\n",
    "C2 = C[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.subplots(figsize=(10,10))\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(C1[labels == i][:1000], C2[labels == i][:1000], label = class_name, alpha=0.4)\n",
    "plt.legend()\n",
    "plt.title(\"PCA Projection\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model2 = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Flatten(input_shape = (x, y, z)),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.leaky_relu),\n",
    "    tf.keras.layers.Dense(50, activation=tf.nn.leaky_relu),\n",
    "\n",
    "    tf.keras.layers.Dense(3, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model2.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(train_features, labels, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = model2.predict(test_features)\n",
    "label_pred  = np.argmax(label_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(labels_val,label_pred))\n",
    "print(classification_report(labels_val,label_pred))\n",
    "print(accuracy_score(labels_val, label_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_loss(history):\n",
    "    \"\"\"\n",
    "        Plot the accuracy and the loss during the training of the nn.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history.history['accuracy'],'bo--', label = \"acc\")\n",
    "    plt.plot(history.history['val_accuracy'], 'ro--', label = \"val_acc\")\n",
    "    plt.title(\"train_acc vs val_acc\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss function\n",
    "    plt.subplot(222)\n",
    "    plt.plot(history.history['loss'],'bo--', label = \"loss\")\n",
    "    plt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\n",
    "    plt.title(\"train_loss vs val_loss\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_loss(history2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect SVM to the PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(C=5,kernel='rbf',tol=1e-7,gamma=0.3)\n",
    "svm_clf.fit(C,labels)\n",
    "import pandas as pd\n",
    "svm = svm_clf.predict(C_test)\n",
    "df_confusion = pd.crosstab(labels_val, svm,rownames=['actual'],colnames=['predicted'])\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using quantitative features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.DataFrame(columns=['size_im_x','size_sq_x','r','g','b','y','cr','cb','un','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_val = pd.DataFrame(columns=['size_im_x','size_sq_x','r','g','b','y','cr','cb','un,''label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "def create_data_pandas(path,df,image_size=150):\n",
    "    \n",
    "    '''\n",
    "    Create a data frame with quantitative features\n",
    "    \n",
    "    '''\n",
    "    valid_images = [\"jpg\",\"gif\",\"png\",\"tga\",\"bmp\"]\n",
    "    label = 0\n",
    "\n",
    "    for i in os.listdir(path):\n",
    "        if os.path.isdir(path + i):\n",
    "            for j in os.listdir(path+i):\n",
    "                if j.split('.')[1] not in valid_images:\n",
    "                    continue\n",
    "                #img = Image.open(path+i+'/'+j).convert('RGB')\n",
    "                img = cv2.imread(path+i+'/'+j)\n",
    "                gray = cv2.imread(path+i+'/'+j,0)\n",
    "                corners = cv2.goodFeaturesToTrack(gray,4,0.2,2,useHarrisDetector=True)\n",
    "                if len(corners) !=4:\n",
    "                    print(j)\n",
    "                im_x,im_y = img.shape[0],img.shape[1]\n",
    "                #img1 = img.crop((corners[1][0][0], corners[1][0][0], corners[1][0][1], corners[1][0][1])).convert('RGB')\n",
    "                crners= np.unique(corners)\n",
    "                crners.sort()\n",
    "                un = np.unique(img)\n",
    "                img = img[int(crners[0]):int(crners[1]), int(crners[0]):int(crners[1])]\n",
    "                sq_x,sq_y = img.shape[0],img.shape[1]\n",
    "                avg_color_per_row = np.average(img, axis=0)\n",
    "                b,g,r = np.average(avg_color_per_row, axis=0)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n",
    "                avg_color_per_row = np.average(img, axis=0)\n",
    "                y,cr,cb = np.average(avg_color_per_row, axis=0)\n",
    "                \n",
    "                df = df.append({'size_im_x':im_x,'size_sq_x':sq_x,'r':r,'g':g,'b':b,'y':y,'cr':cr,'cb':cb,'un':un,'label':label}, ignore_index=True)\n",
    "                #image = cv2.imread(path+i+'/'+j)\n",
    "                #image = np.array(img1)[:, :, ::-1]\n",
    "                #image = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "                # update the raw images, features, and labels matricies,\n",
    "                # respectively\n",
    "\n",
    "            label+=1\n",
    "\n",
    "    #output = []       \n",
    "    #output.append((images, labels))\n",
    "    return df\n",
    "\n",
    "df_tr = create_data_pandas('train/',df_tr)\n",
    "df_val = create_data_pandas('val/',df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df_tr = df_tr.sample(frac=1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_tr = sc.fit_transform(df_tr.iloc[:,:-1].values)\n",
    "X_te = sc.transform(df_val.iloc[:,:-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "regressor = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "regressor.fit(df_tr.iloc[:,:-1].values, df_tr['label'])\n",
    "y_pred = regressor.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(df_val['label'],y_pred))\n",
    "print(classification_report(df_val['label'],y_pred))\n",
    "print(accuracy_score(df_val['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(C=5,kernel='rbf',gamma=0.2,tol=1e-7)\n",
    "svm_clf.fit(X_tr,df_tr['label'])\n",
    "y_pred = svm_clf.predict(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "## We check the misclassified images and analyze them to find any pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mislabeled_images(class_names, test_images, test_labels, pred_labels):\n",
    "    \"\"\"\n",
    "        Print 25 examples of mislabeled images by the classifier, e.g when test_labels != pred_labels\n",
    "    \"\"\"\n",
    "    BOO = (test_labels == pred_labels)\n",
    "    mislabeled_indices = np.where(BOO == 0)\n",
    "    mislabeled_images = test_images[mislabeled_indices]\n",
    "    mislabeled_labels = pred_labels[mislabeled_indices]\n",
    "\n",
    "    title = \"Some examples of mislabeled images by the classifier:\"\n",
    "    display_examples(class_names,  mislabeled_images, mislabeled_labels)\n",
    "print_mislabeled_images(class_names, test_images, labels_val, label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "for j in os.listdir('val/b'):\n",
    "    f.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for i in np.where((labels_val!=label_pred) & (labels_val==0) & (label_pred==1))[0]:\n",
    "    img_name = f[i]\n",
    "    print(img_name)\n",
    "    plt.imshow(cv2.imread('val/a/' + img_name)[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "for i in np.where((labels_val!=label_pred) & (labels_val==1) & (label_pred==0))[0]:\n",
    "    img_name = f[i-500]\n",
    "    print(img_name)\n",
    "    plt.imshow(cv2.imread('val/b/' + img_name)[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = []\n",
    "size = []\n",
    "for i in np.where((labels_val!=label_pred) & (labels_val==1) & (label_pred==0))[0]:\n",
    "    \n",
    "    avg_color_per_row2 = np.average(images_val[i], axis=0)\n",
    "    size.append(images_val[i].shape[0])\n",
    "    avg_color2 = np.average(avg_color_per_row2, axis=0)\n",
    "    results2.append(avg_color2)\n",
    "np_results2 = np.array(results2) # to make results a numpy array\n",
    "plt.hist(np_results2,color=['red','green','blue'])\n",
    "plt.show() # to show the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(df_val['label'],y_pred))\n",
    "print(classification_report(df_val['label'],y_pred))\n",
    "print(accuracy_score(df_val['label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "im = []\n",
    "for i in np.where((df_val['label']!=y_pred) & (df_val['label']==1) & (y_pred==0))[0]:\n",
    "    img_name = f[i-500]\n",
    "    print(img_name)\n",
    "    plt.imshow(cv2.imread('val/b/' + img_name)[:, :, ::-1])\n",
    "    im.append(cv2.imread('val/b/' + img_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis for quantitative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "im2=[]\n",
    "for i in np.where((df_val['label']!=y_pred) & (df_val['label']==0) & (y_pred==1))[0]:\n",
    "    img_name = f[i]\n",
    "    print(img_name)\n",
    "    plt.imshow(cv2.imread('val/a/' + img_name)[:, :, ::-1])\n",
    "    im2.append(cv2.imread('val/a/' + img_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = []\n",
    "size = []\n",
    "for i in np.where((labels_val!=label_pred) & (labels_val==1) & (label_pred==0))[0]:\n",
    "    \n",
    "    avg_color_per_row2 = np.average(images_val[i], axis=0)\n",
    "    size.append(images_val[i].shape[0])\n",
    "    avg_color2 = np.average(avg_color_per_row2, axis=0)\n",
    "    results2.append(avg_color2)\n",
    "np_results2 = np.array(results2) # to make results a numpy array\n",
    "plt.hist(np_results2,color=['red','green','blue'])\n",
    "plt.show() # to show the histogram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
